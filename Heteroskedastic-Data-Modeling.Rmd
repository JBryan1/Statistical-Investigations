---
title: "Heteroskedastic Data Modeling"
author: "Jonathan Bryan"
date: "April 20, 2018"
output: pdf_document
---

## Problem
https://www3.nd.edu/~rwilliam/stats2/l25.pdf

Many statistical models assume constant variance of the error term when modeling a response variable against predictors. Heteroskedasticity breaks this assumption and can present unique challenges to inference and prediction. Heteroskedasticity occurs when the conditional variance of the response $Y$ given the predictors $X$ is also a function of $t$ time or some vairable that has an ordered scale. In other words, the variance of the error term may change over time or over some ordered variable. Classic examples include variation in consumption habits increasing with income (more money to buy various goods) and ___. Also model misspecification.

## Approach

A survey of exploratory data analysis methods is given to show how heteroskedasticity can be observed in data. A comparison of _____ models is given to highlight advantages and disadvantages to different approaches. 

## Detecting Heteroskedascity in Data

We simulate three different datasets, using $p=1,\; p=2,\; p = 10$. We use the $p=10$ dataset to explore the effect of multicollinearity on linear regression when heterskedasticity is present.
```{r message=FALSE}
#load libraries
library(MASS)
library(rmutil)
library(plotly)

#Generate simulated data p=1
set.seed(1)
time = seq(1,100,1)
y = rep(NA,100)

for (i in 1:100){
  sigma = 1 + i*0.1
  y[i] = 10 + time[i] + rnorm(1,0,sigma)
  
}
plot(time,y, main = "Fig.1 Heteroskedastic Data p=1")

#linear model
lm_p1.fit = lm(y ~ time)
```


```{r}
#Generate simulated data p=2
set.seed(1)
x_2 = rnorm(100,0,1)
y = rep(NA,100)

for (i in 1:100){
  sigma = i*0.02
  y[i] = 10 + time[i] + -(1/2)*x_2[i] + rnorm(1,0,sigma)
  
}
plot(x_2,y)
plot_ly(x = time,y = x_2, z=y, type = "contour")
```

```{r}
set.seed(1)
mu = c(60000, 5.5,200000,2.5,2.5,2.5,2.5)
sigma1 = 25000
sigma2 = 0.5
sigma3 = 50000
sigma4 = 0.5
sigma5 = 0.25
sigma6 = 0.23
sigma7 = 0.2

p0 = 0.01
p1 = 0.25
p2 = 0.5
p3 = 0.75

#Correlation matrix
Sigma = matrix(c(sigma1, p0, p3, p1, p2, p1, p1,
                 p0, sigma2, p0, p0, p0, p0, p0,
                 p3, p0 , sigma3, p1, p0, p0, p0,
                 p1, p0, p1, sigma4, p3, p0, p0,
                 p2, p0, p0, p3, sigma5, p2, p0,
                 p1, p0, p0, p0, p2, sigma6, p3,
                 p1, p0, p0, p0, p0, p3, sigma7), nrow=7, ncol=7, byrow=TRUE)

p_10 = mvrnorm(100,mu = mu, Sigma = Sigma)




income = sort(rpareto(100,10,5)*10000)
height = rnorm(100,5.5,1)
home_value = 0.7*income + abs(rt(100,1,1))
impulsivity = rbinom(100,4,0.5)
confidence = rbinom(100,4,0.5)
buyer_score = 1.5*impulsivity + 1.2*married + 2*confidence - 1.3*government_asst
saver_score = 2*married + 1.8*income + 1.5*home_value - 3*impulsivity - 1.2*gender


gender = rbinom(100,1,0.5)
married = rbinom(100,1,0.3) 



government_asst = rbinom(100,1,0.2)



for (i in 1:100){
  sigma = i*0.25
  consumption[i] = 10 + 2*married[i] + .001*income[i] + 20*buyer_score + .0005*home_value[i] - 3*impulsivity[i] - 1.2*gender[i] + rnorm(1,0,sigma)
  
}


p_10 = data.frame(income,
                  gender,
                  married,
                  height,
                  home_value,
                  impulsivity,
                  confidence,
                  government_asst,
                  buyer_score,
                  saver_score,
                  consumption)


plot(income,consumption)



```
http://www.jstor.org.proxy.lib.duke.edu/stable/pdf/2988471.pdf?refreqid=excelsior:40ccf64937d1b684ec45d40dc0c9d3fa

https://rpubs.com/cyobero/187387



*Goldfeld-Quandt test*
This test creates two subsets of the data to run separated ordinary least squares regressions. The separation point of the subsets can be arbitrary and do not have to contain all observations within each subset. The test assumed a normal distribution of errors along with both subsets of the data being full rank. The test statistic is: ________. The Goldfeld-Quandt test statistic is the ratio of the mean square errors or the regressions on the two subsets of the data. Increasing the number of observations dropped in the "middle" of the ordering will increase the power of the test but reduce the degrees of freedom for the test statistic(CITE). A common technique is dropping the middle third of observations with smaller proportions of dropped observations as sample size increases. The Goldfeld-Qaundt test is simple enough, however, it requires the data ordered against a predictor. In addition, the error variance must be a monotonic function of the specified explanatory variable. For example, a quadratic function mapping the explanatory variable to error variance the Goldfeld-Quandt test may improperly accept the null hypothesis of homoskedastic errors (CITE).
```{r}
library(lmtest)
gqtest(lm_p1.fit)
```

*Breusch-Pagan test*

```{r}
bptest(lm_p1.fit)
```
*White test*
```{r}
library(tseries)
white.test(time,y)
```










```{r}
lm.fit = lm(y ~ x)
summary(lm.fit)
par(mfrow=c(2,2))
plot(lm.fit)
```

## Corrective Data Transformations



## Statistical Modeling
https://lib.ugent.be/fulltxt/RUG01/002/376/288/RUG01-002376288_2017_0001_AC.pdf

http://www.stat.columbia.edu/~gelman/research/published/aiepub.pdf


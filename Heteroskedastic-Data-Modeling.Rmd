---
title: "Heteroskedastic Data Modeling"
author: "Jonathan Bryan"
date: "April 20, 2018"
output: pdf_document
---

## Problem
https://www3.nd.edu/~rwilliam/stats2/l25.pdf

Many statistical models assume constant variance of the error term when modeling a response variable against predictors. Heteroskedasticity breaks this assumption and can present unique challenges to inference and prediction. Heteroskedasticity occurs when the conditional variance of the response $Y$ given the predictors $X$ is also a function of $t$ time or some vairable that has an ordered scale. In other words, the variance of the error term may change over time or with an ordered variable. Examples include increasing variation of measurement error of consumption habits with income (more money to buy various goods) and greater error variance of capital expenditures based on company size.

## Approach

We simulate five datasets where $n=300$, $p=1,2,10,100,250$ and where the error term of the model is a linear function of one covariate in each dataset. We then simulate four more datasets where $n=300$ and $p=2,10,100,250$ and half of the covariates are modestly correlated with each other ($0.2 \leq \rho \leq 0.5$) but not including the covariate that is driving the heteroskedasticity, except for in the $p=2$ case. These four datasets will help demonstrate the effect of multicollinearity on detection and modelling of heterskedastic data.

A survey of exploratory data analysis methods and test statistics available in R is given to show how heteroskedasticity can be detected in low- and high-dimensional data and the effect of multicollinearity on detection. A comparison of _____ regression models is given to highlight advantages and disadvantages to different approaches. 

```{r echo = FALSE, message=FALSE}
###creating simulated data with no collinearity
#load libraries
library(MASS)
library(rmutil)
library(plotly)

#Randomly sample 250 true beta coefficients
set.seed(1)
betas = sample(-10:10, size = 250, replace = TRUE)

#Generate simulated heteroskedastic data with n = 300 p=1
set.seed(130)
time = seq(1,300,1)
y_1 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_1[i] = 10 + betas[1]*time[i] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=2, 2 significant covariates
set.seed(330)
X_2 = matrix(rnorm(300,0,1), ncol = 1, nrow = 300)
y_2 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_2[i] = 10 + betas[1]*time[i] + betas[2]*X_2[i] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=10, 5 significant covariates
set.seed(500)
mu_10 = sample(-10:10, size = 9, replace = TRUE)
sigma_10 = diag(1,9)
X_10 = mvrnorm(300,mu_10,sigma_10, empirical = TRUE)
y_10 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_10[i] = 10 + betas[1]*time[i] + betas[2:6]%*%X_10[i,2:6] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=100, 50 significant covariates
set.seed(12)
mu_100 = sample(-10:10, size = 99, replace = TRUE)
sigma_100 = diag(1,99)
X_100 = mvrnorm(300,mu_100,sigma_100, empirical = TRUE)
y_100 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_100[i] = 10 + betas[1]*time[i] + betas[2:50]%*%X_100[i,2:50] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=250, 125 significant covariates
set.seed(180)
mu_250 = sample(-10:10, size = 249, replace = TRUE)
sigma_250 = diag(1,249)
X_250 = mvrnorm(300,mu_250,sigma_250, empirical = TRUE)
y_250 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_100[i] = 10 + betas[1]*time[i] + betas[2:125]%*%X_250[i,2:125] + rnorm(1,0,sigma)
  
}
```
```{r echo = FALSE}
#Generate simulated heteroskedastic data with n = 300 p=2, 2 significant covariates, and multicollinearity
set.seed(500)
X_2mc = rnorm(300,0,1)
M = cbind(time,X_2mc)
c1 = var(M)
chol1 = solve(chol(c1))
X_2mc =  M %*% chol1 
newc = matrix( 
c(1  , 0.2, 
  0.2, 1), nrow = 2, ncol=2 )
chol2 = chol(newc)
X_2mc = X_2mc %*% chol2 * sd(time)
y_2mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_2mc[i,1]
  y_2mc[i] = 10 + betas[1:2]%*%X_2mc[i,1:2] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=10, 5 significant covariates, and multicollinearity
set.seed(25)
mu_10mc = sample(-10:10, size = 10, replace = TRUE)
sigma_10mc = diag(1,10)
cors_10 = sample(1:20, size = 4)*.01
sigma_10mc[2:5,7:10] = cors_10
sigma_10mc[7:10,2:5] = t(sigma_10mc[2:5,7:10])
sigma_10mc = t(sigma_10mc)%*%sigma_10mc
X_10mc = mvrnorm(300,mu_10mc,sigma_10mc, empirical = TRUE)
X_10mc[,1] = sort(abs(X_10mc[,1]))
y_10mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_10mc[i,1]
  y_10mc[i] = 10 + betas[1:6]%*%X_10mc[i,1:6] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=100, 50 significant covariates, and multicollinearity
set.seed(670)
mu_100mc = sample(-10:10, size = 100, replace = TRUE)
sigma_100mc = diag(1,100)
cors_100 = sample(1:20, size = 49, replace = TRUE)*.01
sigma_100mc[2:50,52:100] = cors_100
sigma_100mc[52:100,2:50] = t(sigma_100mc[2:50,52:100])
sigma_100mc = t(sigma_100mc)%*%sigma_100mc
X_100mc = mvrnorm(300,mu_100mc,sigma_100mc, empirical = TRUE)
X_100mc[,1] = sort(abs(X_100mc[,1]))
y_100mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_100mc[i,1]
  y_100mc[i] = 10 + betas[1:50]%*%X_100mc[i,1:50] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=250, 125 significant covariates, and multicollinearity
set.seed(32)
mu_250mc = sample(-10:10, size = 250, replace = TRUE)
sigma_250mc = diag(1,250)
cors_250 = sample(1:20, size = 124, replace = TRUE)*.01
sigma_250mc[2:125,127:250] = cors_250 
sigma_250mc[127:250,2:125] = t(sigma_250mc[2:125,127:250]) 
sigma_250mc = t(sigma_250mc)%*%sigma_250mc
X_250mc = mvrnorm(300,mu_250mc,sigma_250mc, empirical = TRUE)
X_250mc[,1] = sort(abs(X_250mc[,1]))
y_250mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_250mc[i,1]
  y_250mc[i] = 10 + betas[1:125]%*%X_250mc[i,1:125] + rnorm(1,0,sigma)
  
}
```

4) Heteroskedasticity tests: all tests with R support
5) Corrective Data Transformations
6) Regression modeling: low vs. high dimension and no vs. mullticol for each
7) Classification modeling: low vs. high dimension and no vs. mullticol for each

## Visual Detection of Heteroskedasticity

*Response-Predictor Plots*
When using only one predictor to model the response, we can directly observe the behavior of the response variable along the scale of the predictor. We simulate a simple low dimensional dataset ($n=300, p=1$), specified as $Y \sim N(\mu, \sigma^2)$ where $\mu = 10 + time*\beta$, $\beta=2$, $\sigma^2 = 1.5*time$ where the error term is indexed by a multiple of time. Notice that the dispersion of the $Y$ response increase with time.

```{r echo = FALSE, message=FALSE}
#load libraries
library(MASS)
library(rmutil)
library(plotly)

#plot p = 1 heteroskedastic data
plot(time,y_1, main = "Fig.1 Heteroskedastic Data p=1",
     ylab = "y")
```

For multiple regression, rather than looking at several different plots of each variable against the response, residuals plots can be used to detect non-constant variance of the error term. If we oberve that the the variance of the residuals is increasing along the scale of the fitted values, this is a good indication that heterskedasticity is present in the data. We see that as $p \to n$ it is harder to visually detect heteroskedasticty. In addition, when there is even modest multicollinearity among the covariates ($p < 0.05*n$), visual detection using residual plots becomes nearly impossible.

```{r echo = FALSE}
#plot p = 2,10,100,250 heteroskedastic data and no multicollinearity
lm_1 = lm(y_1 ~ time)
lm_2 = lm(y_2 ~ time + X_2)
lm_10 = lm(y_10 ~ time + X_10)
lm_100 = lm(y_100 ~ time + X_100)
lm_250 = lm(y_100 ~ time + X_250)

par(mfrow=c(2,2))
plot(lm_2, which=c(1,1), caption ="p = 2")
plot(lm_10, which=c(1,1),caption ="p = 10")
plot(lm_100, which=c(1,1), caption ="p = 100")
plot(lm_250, which=c(1,1), caption ="p = 250")

#plot p = 10,100,250 heteroskedastic data and multicollinearity
lm_2mc = lm(y_2mc ~ X_2mc)
lm_10mc = lm(y_10mc ~ X_10mc)
lm_100mc = lm(y_100mc ~ X_100mc)
lm_250mc = lm(y_250mc ~ X_250mc)

par(mfrow=c(2,2))
plot(lm_2mc, which=c(1,1), caption ="p = 2 with multicollinearity")
plot(lm_10mc, which=c(1,1), caption ="p = 10 with multicollinearity")
plot(lm_100mc, which=c(1,1), caption ="p = 100 with multicollinearity")
plot(lm_250mc, which=c(1,1), caption ="p = 250 with multicollinearity")
```

## Statistical Tests of Heteroskedasticity

*Goldfeld-Quandt test*

This test creates two subsets of the data to run separate ordinary least squares regressions. The separation point of the subsets can be arbitrary and do not have to contain all observations within each subset. The test assumes Gaussian errors and that both subsets of the data are full rank. The full rank specification implies $p < n/2$ such that if the data is in higher dimensions the matrix subsets become ill-posed.

The Goldfeld-Quandt test statistic is the ratio of the mean square errors of the regressions on the two subsets of the data, the intution is that it tests to see if the variance of both models are significantly different. The test statistic is:$$\frac{MSE_{1}}{MSE_{2}} \sim F_{n_1-k,\;n_2-k}$$ 

Where $MSE_{1}$ and $MSE_{2}$ are the mean sqaured error of the first and second half of the data respectively. The null hypothesis follows and F distribution with $n_1-k,\;n_2-k$ degrees of freedom. Increasing the number of observations dropped in the "middle" of the ordering will increase the power of the test but reduce the degrees of freedom for the test statistic(CITE).

The Goldfeld-Qaundt test requires the be data ordered against a predictor. For this reason, the Goldfeld-Qaundt test may be an expensive procedure if there are many predictors in the model, as each predictor must be ordered and then tested. In addition, the test assumes the variance of the error term must be a monotonic function of the specified explanatory variable. For example, a if the variance of the error term is a nonlinar function of the explanatory variable the Goldfeld-Quandt test may erroneously accept the null hypothesis of homoskedastic errors (CITE).

Advantages

- Low cost test procedure for small $p$
- F-distribution is the asymptotic sampling distribution of the test statistic

Disadvantages

- Requires target explanatory variable to be ordered so computational cost will increase with $p$
- Does not detect heteroskedasticty contributed by unknown variables
- Assumes variance of the error term is a monotonic function of the explanatory variable
- Breaks down when $p > n/2$ 


```{r echo = FALSE, message = FALSE}
#Goldfeld-Quandt test
library(lmtest)

#GQ test function
GQ.function = function(lm , time, X){
                              p = dim(X)[2]
                              GQ.test = rep(NA, 1 + p)
                              GQ.test[1] = gqtest(lm, order.by = time)[[5]]

                              for (i in 2:(p+1)){
                                             GQ.test[i] = gqtest(lm, order.by = X[,i-1])[[5]]
                              }
                      GQ.test        
}
#confirm time tested positive
GQ.time = rep(NA,9)
GQ.time[1] = gqtest(lm_1)[[5]] < .05
GQ.time[2] = GQ.function(lm_2, time, X_2)[1] < .05
GQ.time[3] = GQ.function(lm_10, time, X_10)[1] < .05
GQ.time[4] = GQ.function(lm_100, time, X_100)[1] < .05
GQ.time[5] = NA
GQ.time[6] = GQ.function(lm_2mc, time, X_2mc)[1] < .05
GQ.time[7] = GQ.function(lm_10mc, time, X_10mc)[1] < .05
GQ.time[8] = GQ.function(lm_100, time, X_100mc)[1] < .05
GQ.time[9] = NA

#calculate false positive rate
GQ.fp = rep(NA,9)
GQ.fp[1] = NA
GQ.fp[2] = sum(round(GQ.function(lm_2, time, X_2),3)[2] < .05)/1
GQ.fp[3] = sum(round(GQ.function(lm_10, time, X_10),3)[2:10] < .05)/9
GQ.fp[4] = sum(round(GQ.function(lm_100, time, X_100),3)[2:100] < .05)/99
GQ.fp[5] = NA #breaks down at p=250
GQ.fp[6] = sum(round(GQ.function(lm_2mc, time = X_2mc[,1] , matrix(X_2mc[,2], ncol=1)),3)[2] < .05)/1
GQ.fp[7] = sum(round(GQ.function(lm_10mc, time = X_10mc[,1], matrix(X_10mc[,2:10], ncol=9)),3)[2:10] < .05)/9
GQ.fp[8] = sum(round(GQ.function(lm_100mc, time = X_100mc[,1] , matrix(X_100mc[,2:100], ncol=99)),3)[2:100] < .05)/99
GQ.fp[9] = NA #breaks down at p=250
```


```{r}
#Harrison-McCabe test

#HMC test function
HMC.function = function(lm , time, X){
                              p = dim(X)[2]
                              HMC.test = rep(NA, 1 + p)
                              HMC.test[1] = hmctest(lm, order.by = time)[[3]]

                              for (i in 2:(p+1)){
                                             HMC.test[i] = hmctest(lm, order.by = X[,i-1])[[3]]
                              }
                      HMC.test        
                      }

#confirm time tested positive
HMC.time = rep(NA,9)
HMC.time[1] = gqtest(lm_1)[[5]] < .05
HMC.time[2] = HMC.function(lm_2, time, X_2)[1] < .05
HMC.time[3] = HMC.function(lm_10, time, X_10)[1] < .05
HMC.time[4] = HMC.function(lm_100, time, X_100)[1] < .05
HMC.time[5] = NA
HMC.time[6] = HMC.function(lm_2mc, time, X_2mc)[1] < .05
HMC.time[7] = HMC.function(lm_10mc, time, X_10mc)[1] < .05
HMC.time[8] = HMC.function(lm_100, time, X_100mc)[1] < .05
HMC.time[9] = NA

#calculate false positive rate
HMC.fp = rep(NA,9)
HMC.fp[1] = NA
HMC.fp[2] = sum(round(HMC.function(lm_2, time, X_2),3)[2] < .05)/1
HMC.fp[3] = sum(round(HMC.function(lm_10, time, X_10),3)[2:10] < .05)/9
HMC.fp[4] = sum(round(HMC.function(lm_100, time, X_100),3)[2:100] < .05)/99
HMC.fp[5] = NA #breaks down at p=250
HMC.fp[6] = sum(round(HMC.function(lm_2mc, time = X_2mc[,1] , matrix(X_2mc[,2], ncol=1)),3)[2] < .05)/1
HMC.fp[7] = sum(round(HMC.function(lm_10mc, time = X_10mc[,1], matrix(X_10mc[,2:10], ncol=9)),3)[2:10] < .05)/9
HMC.fp[8] = sum(round(HMC.function(lm_100mc, time = X_100mc[,1] , matrix(X_100mc[,2:100], ncol=99)),3)[2:100] < .05)/99
HMC.fp[9] = NA #breaks down at p=250
```

```{r}
df = matrix(c(GQ.time,HMC.time),
            byrow = TRUE,
            ncol = 9,
            nrow = 2)
df = data.frame(df, row.names = c("GQ",
                             "HMC"
                             )
           )

df2 = matrix(round(c(GQ.fp,HMC.fp),3),
            byrow = TRUE,
            ncol = 9,
            nrow = 2)
df2 = data.frame(df2, row.names = c("GQ False Positive Rate",
                             "HMC False Positive Rate"
                             )
           )
colnames(df) = c(1,2,10,100,250,"2mc","10mc","100mc", "250mc")
colnames(df2) = c(1,2,10,100,250,"2mc","10mc","100mc", "250mc")
knitr::kable(df, caption = "Table 1. GQ and HMC True Positive Result for Time Variable")
knitr::kable(df2, caption = "Table 2. GQ and HMC False Positive Rate")
```

```{r}
#Breusch-Pagan test
BP.test = rep(NA,9)
BP.test[1] = round(bptest(lm_1)[[4]],3)
BP.test[2] = round(bptest(lm_2)[[4]],3)
BP.test[3] = round(bptest(lm_10)[[4]],3)
BP.test[4] = round(bptest(lm_100)[[4]],3)
BP.test[5] = round(bptest(lm_250)[[4]],3)
BP.test[6] = round(bptest(lm_2mc)[[4]],3)
BP.test[7] = round(bptest(lm_10mc)[[4]],3)
BP.test[8] = round(bptest(lm_100mc)[[4]],3)
BP.test[9] = round(bptest(lm_250mc)[[4]],3)

BP.test = matrix(BP.test, ncol = 9, nrow = 1)
BP.test = data.frame("BP Test" = BP.test)
colnames(BP.test) = c(1,2,10,100,250,"2mc","10mc","100mc", "250mc")
knitr::kable(BP.test, caption = "Table 3. Breusch-Pagan P-Values")
```

*Harrison-McCabe test*

The Harrison-McCabe test is similar to the Goldfeld-Qaundt test, in that an arbitrary fractional breakpoint is first selected. The test statistic then calculates the divergence of the ratio of the residual sum of squares for each half of the residuals from the selected fractional breakpoint. The null hypothesis $H_0: b = \frac{RSS_{<b}}{RSS}$ is asymptotically bounded by Beta distributions that are a function of $n$ , $p$ and the number of observations below the breakpoint. Harrison-McCabe test also requires that the target predictor be ordered, which could be an expensive operation in high dimensions.

Advantages

- Simpler than the Goldfeld-Quandt test
- Test statistic is asymptotically bounded by Beta distributions 
- Offers more power compared to Goldfeld-Quandt test if the target predictor is "evenly"" spaced

Disadvantages

- Requires target explanatory variable to be ordered so computational cost will increase with $p$
- Does not detect heteroskedasticty contributed by unknown variables
- Assumes variance of the error term is a monotonic function of the explanatory variable
- Breaks down when $p > n/2$ 

*Breusch-Pagan test*


An extension of the Breusch-Pagan test is the White test that extends the linear model to include interaction and quadratic transformations of the currtent model covariates, thus a more generalized approach to detect heteroskedasticty that is not a linear function of the covariates. However, when $n$ is near $p$ in size, as often the case in high-dimensions, the White test proves problematic when adding additional predictors to the model and $p > n$. 

http://www.jstor.org.proxy.lib.duke.edu/stable/pdf/2988471.pdf?refreqid=excelsior:40ccf64937d1b684ec45d40dc0c9d3fa

https://rpubs.com/cyobero/187387


## Corrective Data Transformations



## Statistical Modeling

In the linear regression setting, the maximum likelihood estimates of the coefficients remain unbiased in the presence of heteroskedasticity, however the standard error estimates are not which can effect whether a explanatory predictor is found to be significant. We see below that even with modest heterskedasticity our coefficient estimate and significance test for the time variable reflect the true model. However, when we increase the dimension of our data by one variable $p=2$, we no longer have an accurate coefficient estimate for the additional variable and the only significant variable is time. When we further increase the dimensions of the data to $p=10$ we lose two more signficant predictors.
```{r}
#summary
#stargazer::stargazer(lm_p1.fit)
```

### Regression models
https://lib.ugent.be/fulltxt/RUG01/002/376/288/RUG01-002376288_2017_0001_AC.pdf


## Classification models
http://www.stat.columbia.edu/~gelman/research/published/aiepub.pdf


---
title: "Heteroskedastic Data Modeling"
author: "Jonathan Bryan"
date: "April 20, 2018"
output: pdf_document
---

## Problem
https://www3.nd.edu/~rwilliam/stats2/l25.pdf

Many statistical models assume constant variance of the error term when modeling a response variable against predictors. Heteroskedasticity breaks this assumption and can present unique challenges to inference and prediction. Heteroskedasticity occurs when the conditional variance of the response $Y$ given the predictors $X$ is also a function of $t$ time or some vairable that has an ordered scale. In other words, the variance of the error term may change over time or with an ordered variable. Examples include increasing variation of measurement error of consumption habits with income (more money to buy various goods) and greater error variance of capital expenditures based on company size.

## Approach

We simulate five datasets where $n=300$ and $p=1,10,100,250$ and where the error term of the model is a linear function of one covariate in each dataset. We then simulate four more datasets where $n=300$ and $p=2,10,100,250$ and half of the covariates are modestly correlated with each other ($\rho \approx 0.4$) but not including the covariate that is driving the heteroskedasticity, except for in the $p=2$ case. These four datasets will help demonstrate the effect of multicollinearity on detection and modelling of heterskedastic data.

A survey of exploratory data analysis methods and test statistics is given to show how heteroskedasticity can be detected in low- and high-dimensional data (POSSIBLY MULTICOLLINEARITY TOO). A comparison of _____ regression models is given to highlight advantages and disadvantages to different approaches. 

1) Create p=1,2,10,100,250 data sets
```{r message=FALSE}
###creating simulated data with no collinearity
#load libraries
library(MASS)
library(rmutil)
library(plotly)

#Randomly sample 250 true beta coefficients
set.seed(1)
betas = sample(-10:10, size = 250, replace = TRUE)

#Generate simulated heteroskedastic data with n = 300 p=1
set.seed(130)
time = seq(1,300,1)
y_1 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_1[i] = 10 + betas[1]*time[i] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=2, 2 significant covariates
set.seed(330)
X_2 = rnorm(300,0,1)
y_2 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_2[i] = 10 + betas[1]*time[i] + betas[2]*X_2[i] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=10, 5 significant covariates
set.seed(500)
mu_10 = sample(-10:10, size = 9, replace = TRUE)
sigma_10 = diag(1,9)
X_10 = mvrnorm(300,mu_10,sigma_10, empirical = TRUE)
y_10 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_10[i] = 10 + betas[1]*time[i] + betas[2:6]%*%X_10[i,2:6] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=100, 50 significant covariates
set.seed(12)
mu_100 = sample(-10:10, size = 99, replace = TRUE)
sigma_100 = diag(1,99)
X_100 = mvrnorm(300,mu_100,sigma_100, empirical = TRUE)
y_100 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_100[i] = 10 + betas[1]*time[i] + betas[2:50]%*%X_100[i,2:50] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=250, 125 significant covariates
set.seed(180)
mu_250 = sample(-10:10, size = 249, replace = TRUE)
sigma_250 = diag(1,249)
X_250 = mvrnorm(300,mu_250,sigma_250, empirical = TRUE)
y_250 = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*time[i]
  y_100[i] = 10 + betas[1]*time[i] + betas[2:125]%*%X_250[i,2:125] + rnorm(1,0,sigma)
  
}
```

```{r}
X_2 = rnorm(300,0,1)
M = cbind(time,T)
c1 = var(M)
chol1 = solve(chol(c1))
X_2 =  M %*% chol1 
newc <- matrix( 
c(1  , 0.2, 
  0.2, 1), nrow = 2, ncol=2 )
chol2 = chol(newc)
finalx <- X_2 %*% chol2 * sd(time)
```


2) Create p=2, 10,100,250 multicollinearity data sets
```{r}
#Generate simulated heteroskedastic data with n = 300 p=2, 2 significant covariates, and multicollinearity
set.seed(500)
X_2mc = rnorm(300,0,1)
M = cbind(time,X_2mc)
c1 = var(M)
chol1 = solve(chol(c1))
X_2mc =  M %*% chol1 
newc = matrix( 
c(1  , 0.2, 
  0.2, 1), nrow = 2, ncol=2 )
chol2 = chol(newc)
X_2mc = X_2mc %*% chol2 * sd(time)
y_2mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_2mc[i,1]
  y_2mc[i] = 10 + betas[1:2]%*%X_2mc[i,1:2] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=10, 5 significant covariates, and multicollinearity
set.seed(25)
mu_10mc = sample(-10:10, size = 10, replace = TRUE)
sigma_10mc = diag(1,10)
cors_10 = sample(1:20, size = 4)*.01
sigma_10mc[2:5,7:10] = cors_10
sigma_10mc[7:10,2:5] = t(sigma_10mc[2:5,7:10])
sigma_10mc = t(sigma_10mc)%*%sigma_10mc
X_10mc = mvrnorm(300,mu_10mc,sigma_10mc, empirical = TRUE)
X_10mc[,1] = sort(abs(X_10mc[,1]))
y_10mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_10mc[i,1]
  y_10mc[i] = 10 + betas[1:6]%*%X_10mc[i,1:6] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=100, 50 significant covariates, and multicollinearity
set.seed(670)
mu_100mc = sample(-10:10, size = 100, replace = TRUE)
sigma_100mc = diag(1,100)
cors_100 = sample(1:20, size = 49, replace = TRUE)*.01
sigma_100mc[2:50,52:100] = cors_100
sigma_100mc[52:100,2:50] = t(sigma_100mc[2:50,52:100])
sigma_100mc = t(sigma_100mc)%*%sigma_100mc
X_100mc = mvrnorm(300,mu_100mc,sigma_100mc, empirical = TRUE)
X_100mc[,1] = sort(abs(X_100mc[,1]))
y_100mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_100mc[i,1]
  y_100mc[i] = 10 + betas[1:50]%*%X_100mc[i,1:50] + rnorm(1,0,sigma)
  
}

#Generate simulated heteroskedastic data with n = 300 p=250, 125 significant covariates, and multicollinearity
set.seed(32)
mu_250mc = sample(-10:10, size = 250, replace = TRUE)
sigma_250mc = diag(1,250)
cors_250 = sample(1:20, size = 124, replace = TRUE)*.01
sigma_250mc[2:125,127:250] = cors_250 
sigma_250mc[127:250,2:125] = t(sigma_250mc[2:125,127:250]) 
sigma_250mc = t(sigma_250mc)%*%sigma_250mc
X_250mc = mvrnorm(300,mu_250mc,sigma_250mc, empirical = TRUE)
X_250mc[,1] = sort(abs(X_250mc[,1]))
y_250mc = rep(NA,300)

for (i in 1:300){
  sigma = 1.5*X_250mc[i,1]
  y_250mc[i] = 10 + betas[1:125]%*%X_250mc[i,1:125] + rnorm(1,0,sigma)
  
}
```


3) Visual Detection: plotting, residuals plots
4) Heteroskedasticity tests: all tests with R support
5) Corrective Data Transformations
6) Regression modeling: low vs. high dimension and no vs. mullticol for each
7) Classification modeling: low vs. high dimension and no vs. mullticol for each

## Visual Detection of Heteroskedasticity

*Response-Predictor Plots*
When using only one predictor to model the response, we can directly observe the behavior of the response variable along the scale of the predictor. We simulate a simple low dimensional dataset ($n=300, p=1$), specified as $Y \sim N(\mu, \sigma^2)$ where $\mu = 10 + time*\beta$, $\beta=2$, $\sigma^2 = 1.5*time$ and the error term is indexed by time. Notice that the dispersion of the $Y$ response increase with time.

```{r message=FALSE}
#load libraries
library(MASS)
library(rmutil)
library(plotly)

#plot p = 1 heteroskedastic data
plot(time,y_1, main = "Fig.1 Heteroskedastic Data p=1",
     ylab = "y")
```

For multiple regression, rather than looking at several different plot of each variable against the response, residuals plots can be used to detect non-constance variance of the error term. If we oberve that the the variance of the residuals is increasing along the scale of the fitted values, this is a good indication that heterskedasticity is present in the data. We see that as $p \to n$ it is harder to visually detect heteroskedasticty. In addition, when there is even modest multicollinearity among the covariates, visual detection using residual plots becomes nearly impossible 
```{r}
#plot p = 2,10,100,250 heteroskedastic data and no multicollinearity
lm_1 = lm(y_1 ~ time)
lm_2 = lm(y_2 ~ time + X_2)
lm_10 = lm(y_10 ~ time + X_10)
lm_100 = lm(y_100 ~ time + X_100)
lm_250 = lm(y_100 ~ time + X_250)

par(mfrow=c(2,2))
plot(lm_2, which=c(1,1), caption ="p = 2")
plot(lm_10, which=c(1,1),caption ="p = 10")
plot(lm_100, which=c(1,1), caption ="p = 100")
plot(lm_250, which=c(1,1), caption ="p = 250")

#plot p = 10,100,250 heteroskedastic data and multicollinearity
lm_2mc = lm(y_2mc ~ time + X_2mc)
lm_10mc = lm(y_10mc ~ time + X_10mc)
lm_100mc = lm(y_100mc ~ time + X_100mc)
lm_250mc = lm(y_250mc ~ time + X_250mc)

par(mfrow=c(2,2))
plot(lm_2mc, which=c(1,1), caption ="p = 2 with multicollinearity")
plot(lm_10mc, which=c(1,1), caption ="p = 10 with multicollinearity")
plot(lm_100mc, which=c(1,1), caption ="p = 100 with multicollinearity")
plot(lm_250mc, which=c(1,1), caption ="p = 250 with multicollinearity")
```

*Goldfeld-Quandt test*
This test creates two subsets of the data to run separated ordinary least squares regressions. The separation point of the subsets can be arbitrary and do not have to contain all observations within each subset. The test assumes a normal distribution of errors along with both subsets of the data being full rank. The Goldfeld-Quandt test statistic is the ratio of the mean square errors of the regressions on the two subsets of the data. The test statistic is:$$\frac{MSE_{1}}{MSE_{2}} \sim F_{n_1-k,\;n_2-k}$$ 

Where $MSE_{1}$ and $MSE_{2}$ are the mean sqaured error of the first and second half of the data respectively. Increasing the number of observations dropped in the "middle" of the ordering will increase the power of the test but reduce the degrees of freedom for the test statistic(CITE). A common technique is dropping the middle third of observations with smaller proportions of dropped observations as sample size increases. The Goldfeld-Qaundt test is simple enough, however, it requires the data ordered against a predictor. In addition, the test assumes the variance of the error term must be a monotonic function of the specified explanatory variable. For example, a if the variance of the error term isa nonlinar function of the explanatory variable the Goldfeld-Quandt test may erroneously accept the null hypothesis of homoskedastic errors (CITE).

Advantages

- Simple test procedure and statistic
- F-distribution is the asymptotic sampling distribution of the test statistic

Disadvantages

- Requires target explanatory variable to be ordered
- Does not detect heteroskedasticty contributes by an unknown variables
- Assumes variance of the error term is a monotonic function of the explanatory variable



```{r message = FALSE}
#Goldfeld-Quandt test
library(lmtest)
gqtest(lm_1)
gqtest(lm_2)
gqtest(lm_10)
gqtest(lm_100)
gqtest(lm_250)
gqtest(lm_2mc)
gqtest(lm_10mc)
gqtest(lm_100mc)
gqtest(lm_250mc)
```

*Harrison-McCabe test*
```{r}
hmctest(lm_1)
hmctest(lm_2)
hmctest(lm_10)
hmctest(lm_100)
hmctest(lm_250)
hmctest(lm_2mc)
hmctest(lm_10mc)
hmctest(lm_100mc)
hmctest(lm_250mc)
```

*Breusch-Pagan test*
```{r}
bptest(lm_1)
bptest(lm_2)
bptest(lm_10)
bptest(lm_100)
bptest(lm_250)
bptest(lm_2mc)
bptest(lm_10mc)
bptest(lm_100mc)
bptest(lm_250mc)
```


*White test*
```{r}
white.test()
```

Why it breaksdown in high dimensions


http://www.jstor.org.proxy.lib.duke.edu/stable/pdf/2988471.pdf?refreqid=excelsior:40ccf64937d1b684ec45d40dc0c9d3fa

https://rpubs.com/cyobero/187387


## Corrective Data Transformations



## Statistical Modeling

In the linear regression setting, the maximum likelihood estimates of the coefficients remain unbiased in the presence of heteroskedasticity, however the standard error estimates are not which can effect whether a explanatory predictor is found to be significant. We see below that even with modest heterskedasticity our coefficient estimate and significance test for the time variable reflect the true model. However, when we increase the dimension of our data by one variable $p=2$, we no longer have an accurate coefficient estimate for the additional variable and the only significant variable is time. When we further increase the dimensions of the data to $p=10$ we lose two more signficant predictors.
```{r}
#summary
stargazer::stargazer(lm_p1.fit)
```

### Regression models
https://lib.ugent.be/fulltxt/RUG01/002/376/288/RUG01-002376288_2017_0001_AC.pdf


## Classification models
http://www.stat.columbia.edu/~gelman/research/published/aiepub.pdf

